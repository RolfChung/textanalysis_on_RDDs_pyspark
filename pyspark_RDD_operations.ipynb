{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summmary\n",
    "\n",
    "\n",
    "<p>\n",
    "This project focus on Resilient Distributed Datasets or RDDs of the open-source engine Apache Spark developed specifically for handling large-scale data processing and analytics. RDDs are the unique native core data structure of Apache Spark, which allows to process data on numerous remote worker machines.\n",
    "RDDs are partioned and distributed on the nodes in the Spark cluster.\n",
    "The RDDs are resilient and are retrieved from the data existing in other nodes  even when processes are crashing or nodes are failing .\n",
    "</p>\n",
    "<p>\n",
    "According to the \n",
    "<a href=\"https://spark.apache.org/docs/0.8.1/api/core/org/apache/spark/rdd/RDD.html\" target=\"_blank\">documentation</a> \n",
    "a Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel. This class contains the basic operations available on all RDDs, such as map, filter, and persist. In addition, PairRDDFunctions contains operations available only on RDDs of key-value pairs, such as groupByKey and join\n",
    "<p>\n",
    "    \n",
    "<p>\n",
    "There are two types of data sets used here. Numeric data created from random data and the breast cancer data set, and string data randomly choosen from text of the internet. RDD operations are carried out on all data sets. For example a word count is made for the the text data to get the 10 most frequent words.\n",
    "</p>\n",
    "\n",
    "<p>Among many others the following tasks are carried out:</p> \n",
    "        \n",
    "<ul>\n",
    "  <li>Import packages</li>\n",
    "  <li>Inspect SparkContext</li>\n",
    "  <li>Lambda functions</li>    \n",
    "  <li>Creating RDDs by using schemas and parallelize</li>\n",
    "  <li>Selecting data from RDDs</li>\n",
    "  <li>RDD operations</li>  \n",
    "  <li>Creating a dictionary of word counts</li>\n",
    "  <li>Creating a list of top ten words</li>\n",
    "  <li>Using stopwords to filter the text</li>\n",
    "</ul>     \n",
    "    \n",
    "    \n",
    "<p>\n",
    "<h2>Spark system architecture</h2>\n",
    "</p> \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"spark_achitecture.png\" alt=\"Smiley face\" align=\"left\"  style=\"margin-left: 0px; margin-right: 0px; margin-top: 20px; margin-bottom: 20px; float: left; width: 800px; height: 300px\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import pathlib\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import lower, col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql.functions import mean, stddev , col, avg, round\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import isnan\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.mllib.stat import Statistics\n",
    "# throws an annoying Py4JJavaError\n",
    "# it seems there is a compatibility issues between Spark and Java\n",
    "# upgrading Java solve the problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd_1=os.getcwd()\n",
    "# print(cwd_1)\n",
    "\n",
    "dirname = os.path.dirname(cwd_1) \n",
    "# print(dirname)\n",
    "\n",
    "path_cwd = pathlib.Path.cwd()\n",
    "# print(path_cwd)\n",
    "\n",
    "path_home = pathlib.Path.home()\n",
    "# print(path_home)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Spark context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Spark context is a variable automatically created, when starting Pyspark\n",
    "from the shell. It is an entry point into the whole arrangement\n",
    "of Spark functionality. The SparkContext is the entry point into the cluster. Without creating a SparkContext nothing in the Spark session will work. Spark context can be accessed with \"sd\".\n",
    "</p> \n",
    "\n",
    "<p>Furthermore:</p> \n",
    "\n",
    "<p>\n",
    "\"A SparkContext represents the connection to a Spark cluster, \n",
    "and can be used to create RDDs, accumulators and broadcast variables \n",
    "on that cluster.\" \n",
    "</p> \n",
    "<a href=\"https://spark.apache.org/docs/2.0.2/api/java/org/apache/spark/SparkContext.html\" target=\"_blank\">Class SparkContext</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pyspark version running in this session 3.0.0-preview.\n",
      "Python version used in this session of Spark Context is 3.7.\n",
      "Python version used in this session of Spark Context is local[*].\n"
     ]
    }
   ],
   "source": [
    "print(\"Pyspark version running in this session {}.\"\\\n",
    "      .format(sc.version))\n",
    "\n",
    "print(\"Python version used in this session of Spark Context is {}.\"\\\n",
    "      .format(sc.pythonVer))\n",
    "\n",
    "\n",
    "print(\"Python version used in this session of Spark Context is {}.\"\\\n",
    "      .format(sc.master))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamarandor\n",
      "PySparkShell\n",
      "local-1578651229420\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path where Spark is installed on worker nodes\n",
    "# print(str(sc.sparkHome))\n",
    "\n",
    "# Retrieve name of the Spark User running\n",
    "print(str(sc.sparkUser()))\n",
    "\n",
    "# Return application name\n",
    "print(sc.appName)\n",
    "\n",
    "# Retrieve application ID\n",
    "print(sc.applicationId)\n",
    "\n",
    "# Return default level of parallelism \n",
    "print(sc.defaultParallelism)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference of SparkContext and SparkSession\n",
    "\n",
    "From \n",
    "<a href=\"https://data-flair.training/forums/topic/what-is-sparksession-in-apache-spark/\" target=\"_blank\">data-flair</a>:\n",
    "\n",
    "<p>\n",
    "Starting from Apache Spark 2.0, Spark Session is the new entry point for Spark applications.\n",
    "\n",
    "Prior to 2.0, SparkContext was the entry point for spark jobs. RDD was one of the main APIs then, and it was created and manipulated using Spark Context. For every other APIs, different contexts were required – For SQL, SQL Context was required; For Streaming, Streaming Context was required; For Hive, Hive Context was required.\n",
    "\n",
    "But from 2.0, RDD along with DataSet and its subset DataFrame APIs are becoming the standard APIs and are a basic unit of data abstraction in Spark. All of the user defined code will be written and evaluated against the DataSet and DataFrame APIs as well as RDD.\n",
    "\n",
    "So, there is a need for a new entry point build for handling these new APIs, which is why Spark Session has been introduced. Spark Session also includes all the APIs available in different contexts – Spark Context, SQL Context, Streaming Context, Hive Context.\n",
    "</p> \n",
    "\n",
    "<p> \n",
    "SparkSession commands a variety of methods explained on \n",
    "<a href=\"https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/sql/SparkSession.html\" target=\"_blank\">spark.apache.org.</a> \n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x00000124E6D3D7C8>\n",
      "default\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder.getOrCreate()\n",
    "print(spark_session)\n",
    "print(spark_session.catalog.currentDatabase())\n",
    "print(spark_session.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda functions (lambda arguments: expression)\n",
    "\n",
    "as anonymous, inline functions are useful in Python in general but are extensively used in Pyspark and should be part of the tool kit.\n",
    "Only one expression is possible but numerous arguments can be created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map(function, iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "9986\n",
      "[1290, 7294, 1345, 7292, 9373]\n"
     ]
    }
   ],
   "source": [
    "# generate random data\n",
    "# numpy.random.randint(low, high=None, size=None, dtype='l')\n",
    "# makes random numbers predictable\n",
    "np.random.seed(10)\n",
    "lamb_dt = np.random.randint(1,10000, size=1000).tolist()\n",
    "\n",
    "print(min(lamb_dt))\n",
    "print(max(lamb_dt))\n",
    "print(lamb_dt[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5649, 4453, 240, 2444, 2103]\n",
      "[75.15982969645421, 66.73080248281148, 15.491933384829668, 49.4368283772331, 45.858477951192405]\n",
      "[6188, 2752, 2056, 9440, 9414]\n",
      "[127795585653, 27379766744, 633839779000, 17173512, 21300003648]\n"
     ]
    }
   ],
   "source": [
    "# map applies a function on a list and \n",
    "# returns the results as a map object\n",
    "# transformed to list with list\n",
    "# map(function, iteration object like list, dict ...)\n",
    "\n",
    "# take square root of every value in the list\n",
    "list_sqrt = list(map(lambda x: math.sqrt(x), lamb_dt))\n",
    "\n",
    "print(lamb_dt[10:15])\n",
    "print(list_sqrt[10:15])\n",
    "\n",
    "# double every value\n",
    "list_multi = list(map(lambda d: d * 2, lamb_dt))\n",
    "print(list_multi[510:515])\n",
    "\n",
    "# cube every value\n",
    "list_cubed = list(map(lambda e: e**3, lamb_dt))\n",
    "print(list_cubed[900:905])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter(function, iterable)\n",
    "\n",
    "works fine as it returns the values based on the condition\n",
    "made in the lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<filter object at 0x00000124E85FA1C8>\n",
      "991\n",
      "[2815, 535, 3681, 7236, 7264, 4452, 1107, 2111, 3831, 1614, 507, 9265, 9590, 6110, 5870, 460, 2034, 3494, 5014, 9958]\n",
      "104\n",
      "[1290, 4830, 9290, 240, 6900, 410, 7490, 6160, 1160, 6860]\n",
      "506\n",
      "[1345, 9373, 1521, 9225, 6401, 5649, 4453, 2103, 3417, 7291]\n"
     ]
    }
   ],
   "source": [
    "# filter a list based on a greater condition\n",
    "list_greater_map = filter(lambda l: l > 100, lamb_dt)\n",
    "print(list_greater_map)\n",
    "\n",
    "list_greater_list = list(list_greater_map)\n",
    "\n",
    "print(len(list_greater_list))\n",
    "print(list_greater_list[100:120])\n",
    "\n",
    "# filter on the condition that the values can be divided by 10\n",
    "# based on modulo\n",
    "list_mod = list(filter(lambda k: k%10 == 0, lamb_dt))\n",
    "print(len(list_mod))\n",
    "print(list_mod[:10])\n",
    "\n",
    "# filter odd numbers\n",
    "list_mod2 = list(filter(lambda g: g%2 != 0, lamb_dt))\n",
    "print(len(list_mod2))\n",
    "print(list_mod2[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data into Spark \n",
    "\n",
    "\n",
    "<p>\n",
    "with the \"spark.read.csv\", \"parallelize\" and the \"textFile\" method.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "The \"spark.read.csv\" - method create data frames similar to\n",
    "Pandas data frames.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Both other methods are creating Resilient Distributed Datasets (RDDs).\n",
    "RDDs are the unique native core data structure of Apache Spark,\n",
    "which allows to process data on numerous remote worker machines.\n",
    "RDDs are partioned and distributed on the nodes in the Spark cluster.\n",
    "The RDDs are resilient and are retrieved from the data existing in other nodes  even when processes are crashing or nodes are failing .\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Other than SQL-tables and Pandas data frames RDDS are not tabular\n",
    "data structures and relies on NoSQL data models like Key-value-stores.\n",
    "This allows it to work with unstructured data like text very\n",
    "efficient.\n",
    "</p>\n",
    "\n",
    "<a href=\"https://hackersandslackers.com/working-with-pyspark-rdds/?utm_campaign=News&utm_medium=Community&utm_source=DataCamp.com\" target=\"_blank\">Working with PySpark RDDs</a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading csv-data into a Pyspark data frame\n",
    "\n",
    "<p>\n",
    "given by the breast cancer dataset obtained from the University of Wisconsin. The dataset has 11 variables with 699 observations. The first variable is the anonyminous identifier. There are 9 features. \n",
    "The response variable is \"class\". The response has two values: \n",
    "“Malignant” or “Benign” cases.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# there is not a header in the raw data\n",
    "# therefore a schema is created to provided the names\n",
    "# when reading the data in\n",
    "\n",
    "bc_schema = \\\n",
    "StructType([\n",
    "    StructField(\"Sample code number\", IntegerType(), True),\n",
    "    StructField(\"Clump Thickness\", IntegerType(), True),\n",
    "    StructField(\"Uniformity of Cell Size\", IntegerType(), True),\n",
    "    StructField(\"Uniformity of Cell Shape\", IntegerType(), True),\n",
    "    StructField(\"Single Epithelial Cell Size\", IntegerType(), True),\n",
    "    StructField(\"Bare Nuclei\", IntegerType(), True),\n",
    "    StructField(\"Bland Chromatin\", IntegerType(), True),\n",
    "    StructField(\"Normal Nucleoli\", IntegerType(), True),\n",
    "    StructField(\"Mitoses\", IntegerType(), True),\n",
    "    StructField(\"Class\", IntegerType(), True)])\n",
    "\n",
    "\n",
    "bc = \\\n",
    "spark.read.csv(\"breast-cancer-wisconsin.csv\", inferSchema=True, \n",
    "               header=False, schema=bc_schema)\n",
    "\n",
    "print(type(bc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyspark data frames offer a variety of methods to understand\n",
    "the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+---------------+-------+-----+\n",
      "|Bare Nuclei|Bland Chromatin|Normal Nucleoli|Mitoses|Class|\n",
      "+-----------+---------------+---------------+-------+-----+\n",
      "|          2|              1|              3|      1|    1|\n",
      "|          7|             10|              3|      2|    1|\n",
      "|          2|              2|              3|      1|    1|\n",
      "|          3|              4|              3|      7|    1|\n",
      "|          2|              1|              3|      1|    1|\n",
      "+-----------+---------------+---------------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "n observations: 699\n",
      "n cols: 10\n"
     ]
    }
   ],
   "source": [
    "print(bc.select(\"Bare Nuclei\", \"Bland Chromatin\", \n",
    "                \"Normal Nucleoli\", \n",
    "                \"Mitoses\", \"Class\").show(5))\n",
    "\n",
    "print(\"n observations: {}\".format(bc.count()))\n",
    "print(\"n cols: {}\".format(len(bc.columns)))\n",
    "\n",
    "# class is the predictor variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sample code number: integer (nullable = true)\n",
      " |-- Clump Thickness: integer (nullable = true)\n",
      " |-- Uniformity of Cell Size: integer (nullable = true)\n",
      " |-- Uniformity of Cell Shape: integer (nullable = true)\n",
      " |-- Single Epithelial Cell Size: integer (nullable = true)\n",
      " |-- Bare Nuclei: integer (nullable = true)\n",
      " |-- Bland Chromatin: integer (nullable = true)\n",
      " |-- Normal Nucleoli: integer (nullable = true)\n",
      " |-- Mitoses: integer (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+\n",
      "|summary|      Bare Nuclei|           Mitoses|             Class|\n",
      "+-------+-----------------+------------------+------------------+\n",
      "|  count|              699|               699|               699|\n",
      "|   mean|3.216022889842632| 2.866952789699571|1.5894134477825466|\n",
      "| stddev|2.214299886649047|3.0536338936127745| 1.715077942506795|\n",
      "|    min|                1|                 1|                 1|\n",
      "|    max|               10|                10|                10|\n",
      "+-------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bc.select(\"Bare Nuclei\", \"Mitoses\", \"Class\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different methods to select data from a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[833] at javaToPython at <unknown>:0\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "Row(Sample code number=1000025, Clump Thickness=5, Uniformity of Cell Size=1, Uniformity of Cell Shape=1, Single Epithelial Cell Size=1, Bare Nuclei=2, Bland Chromatin=1, Normal Nucleoli=3, Mitoses=1, Class=1)\n"
     ]
    }
   ],
   "source": [
    "bc_rdd = bc.rdd\n",
    "print(bc_rdd)\n",
    "print(type(bc_rdd))\n",
    "\n",
    "# look into the RDD\n",
    "for i in bc_rdd.take(1): \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Sample code number=1000025, Clump Thickness=5, Uniformity of Cell Size=1, Uniformity of Cell Shape=1, Single Epithelial Cell Size=1, Bare Nuclei=2, Bland Chromatin=1, Normal Nucleoli=3, Mitoses=1, Class=1)]\n",
      "\n",
      "Row(Sample code number=1000025, Clump Thickness=5, Uniformity of Cell Size=1, Uniformity of Cell Shape=1, Single Epithelial Cell Size=1, Bare Nuclei=2, Bland Chromatin=1, Normal Nucleoli=3, Mitoses=1, Class=1)\n",
      "\n",
      "[Row(Sample code number=1000025, Clump Thickness=5, Uniformity of Cell Size=1, Uniformity of Cell Shape=1, Single Epithelial Cell Size=1, Bare Nuclei=2, Bland Chromatin=1, Normal Nucleoli=3, Mitoses=1, Class=1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting data\n",
    "print(bc_rdd.take(1))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Take top 5 RDD elements\n",
    "print(bc_rdd.first())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Return distinct RDD values\n",
    "print(bc_rdd.distinct().take(1))\n",
    "\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "104\n",
      "[Row(Sample code number=1016277, Clump Thickness=6, Uniformity of Cell Size=8, Uniformity of Cell Shape=8, Single Epithelial Cell Size=1, Bare Nuclei=3, Bland Chromatin=4, Normal Nucleoli=3, Mitoses=7, Class=1)]\n"
     ]
    }
   ],
   "source": [
    "sample = bc_rdd.sample(False, 0.15, 81).collect()\n",
    "\n",
    "print(type(sample))\n",
    "print(len(sample))\n",
    "print(sample[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "5\n",
      "[5, 5, 3, 6, 4]\n"
     ]
    }
   ],
   "source": [
    "# selecting a column by index\n",
    "# for selecting a column by name better use sqlContext\n",
    "\n",
    "thick = bc_rdd.map(lambda x: x[1])\n",
    "\n",
    "print(type(thick))\n",
    "print(thick.first())\n",
    "print(thick.take(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading numeric data with \"parallelize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[438 198 671 754  48 206 697 348 291 910 354  24 644 577 449 849 813 708\n",
      " 600 350 838 411  34 341 950 342 341  94 767 208 646  75 986 366 735 711\n",
      " 948  33 887 287 508 604 883 646 618 455 393 854 701 137]\n",
      "[1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# generate random data\n",
    "# numpy.random.randint(low, high=None, size=None, dtype='l')\n",
    "# makes random numbers predictable\n",
    "np.random.seed(0)\n",
    "data_1 = np.random.randint(1,1000, size=1000).tolist()\n",
    "print(type(data_1))\n",
    "print(np.random.choice(data_1, size=50, replace=True))\n",
    "\n",
    "data_2 = range(1,1001)\n",
    "print([i for i in data_2[:6]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data into Spark with parallelize \n",
    "creates Resilient Distributed Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[845] at readRDDFromFile at PythonRDD.scala:247\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "PythonRDD[847] at RDD at PythonRDD.scala:53\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "# this creates a pointer object in memory with an address\n",
    "spark_dt_1 = sc.parallelize(data_1)\n",
    "print(spark_dt_1)\n",
    "print(type(spark_dt_1))\n",
    "\n",
    "# this creates a pointer object in memory with an address\n",
    "spark_dt_2 = sc.parallelize(data_2)\n",
    "print(spark_dt_2)\n",
    "print(type(spark_dt_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object is RDD\n",
      "None\n",
      "object is RDD\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def check_type(object):\n",
    "    \n",
    "    if isinstance(object, DataFrame):\n",
    "        print(\"object is DataFrame\")\n",
    "        \n",
    "    if isinstance(object, RDD):\n",
    "        print(\"object is RDD\")\n",
    "            \n",
    "    else:\n",
    "        print(\"other type\")\n",
    "        \n",
    "\n",
    "print(check_type(spark_dt_1))\n",
    "print(check_type(spark_dt_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first values in the RDDs (similar to the head method in R or pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685\n",
      "560\n",
      "630\n",
      "193\n",
      "836\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# how to print out snippets of a RDD in the spark-shell / pyspark?\n",
    "# simulate pandas head\n",
    "# https://stackoverflow.com/questions/31115892/how-to-print-out-snippets-of-a-rdd-in-the-spark-shell-pyspark\n",
    "\n",
    "for i in spark_dt_1.take(5): \n",
    "    print(i)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "for d in spark_dt_2.take(5):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[634, 640, 287, 803, 769, 210, 892, 30, 641, 711, 79, 533]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return sampled subset of rdd3\n",
    "# sample(self, withReplacement, fraction, seed=None)\n",
    "\n",
    "spark_dt_1.sample(withReplacement=False, fraction=0.01, \n",
    "                  seed=15).collect()\n",
    "\n",
    "# collect retrieves all data in the RDD\n",
    "# careful if the RDD is large\n",
    "# https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd.RDD-class.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[851] at readRDDFromFile at PythonRDD.scala:247\n",
      "b\n",
      "r\n",
      "e\n",
      "a\n",
      "s\n",
      "t\n",
      "-\n",
      "c\n",
      "a\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "# this creates a pointer object\n",
    "bc_csv_rdd = sc.parallelize(\"breast-cancer-wisconsin.csv\")\n",
    "print(bc_csv_rdd)\n",
    "\n",
    "# the pointer object can be accessed with a for loop\n",
    "for o in bc_csv_rdd.take(10):\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the RDD to a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 699 entries, 0 to 698\n",
      "Data columns (total 10 columns):\n",
      "Sample code number             683 non-null float64\n",
      "Clump Thickness                683 non-null float64\n",
      "Uniformity of Cell Size        683 non-null float64\n",
      "Uniformity of Cell Shape       683 non-null float64\n",
      "Single Epithelial Cell Size    683 non-null float64\n",
      "Bare Nuclei                    683 non-null float64\n",
      "Bland Chromatin                683 non-null float64\n",
      "Normal Nucleoli                683 non-null float64\n",
      "Mitoses                        683 non-null float64\n",
      "Class                          683 non-null float64\n",
      "dtypes: float64(10)\n",
      "memory usage: 54.7 KB\n",
      "None\n",
      "\n",
      "   Sample code number  Clump Thickness  Uniformity of Cell Size  \\\n",
      "0           1000025.0              5.0                      1.0   \n",
      "1           1002945.0              5.0                      4.0   \n",
      "2           1015425.0              3.0                      1.0   \n",
      "3           1016277.0              6.0                      8.0   \n",
      "4           1017023.0              4.0                      1.0   \n",
      "\n",
      "   Uniformity of Cell Shape  Single Epithelial Cell Size  Bare Nuclei  \\\n",
      "0                       1.0                          1.0          2.0   \n",
      "1                       4.0                          5.0          7.0   \n",
      "2                       1.0                          1.0          2.0   \n",
      "3                       8.0                          1.0          3.0   \n",
      "4                       1.0                          3.0          2.0   \n",
      "\n",
      "   Bland Chromatin  Normal Nucleoli  Mitoses  Class  \n",
      "0              1.0              3.0      1.0    1.0  \n",
      "1             10.0              3.0      2.0    1.0  \n",
      "2              2.0              3.0      1.0    1.0  \n",
      "3              4.0              3.0      7.0    1.0  \n",
      "4              1.0              3.0      1.0    1.0  \n"
     ]
    }
   ],
   "source": [
    "bc_df_45 = sqlContext.createDataFrame(bc_rdd, bc_schema)\n",
    "print(type(bc_df_45 ))\n",
    "\n",
    "bc_df_45_pd = bc_df_45.toPandas()\n",
    "print(type(bc_df_45_pd))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(bc_df_45_pd.info())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(bc_df_45_pd.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading string data with \"textFile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object: OnSunsetHighways.txt MapPartitionsRDD[861] at textFile at <unknown>:0.\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# filepath is short as the file is in the same directory as the notebook\n",
    "# it is book about roundtrips in the U.S.A. around 1912\n",
    "\n",
    "highway = sc.textFile('OnSunsetHighways.txt')\n",
    "\n",
    "print(\"object: {}.\".format(highway))\n",
    "print(type(highway))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10387\n"
     ]
    }
   ],
   "source": [
    "# print number of rows or observations\n",
    "print(highway.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ON SUNSET HIGHWAYS ***\n"
     ]
    }
   ],
   "source": [
    "print(highway.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ON SUNSET HIGHWAYS ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Melissa McDaniel and the Online Distributed\n",
      "Proofreading Team at http://www.pgdp.net (This file was\n",
      "produced from images generously made available by The\n",
      "Internet Archive. The map and cover are courtesy of the\n",
      "California History Room, California State Library,\n",
      "Sacramento, California.)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Transcriber's Note:\n",
      "\n",
      "  Inconsistent hyphenation in the original document has been\n",
      "  preserved. Obvious typographical errors have been corrected.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "     ON SUNSET\n",
      "     HIGHWAYS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"SEE AMERICA FIRST\" SERIES\n",
      "\n",
      "Each in one volume, decorative cover, profusely illustrated\n",
      "\n",
      "\n",
      "     CALIFORNIA, ROMANTIC AND BEAUTIFUL\n",
      "               BY GEORGE WHARTON JAMES                 $6.00\n",
      "\n",
      "     NEW MEXICO: The Land of the Delight Makers\n",
      "               BY GEORGE WHARTON JAMES                 $6.00\n",
      "\n",
      "     SEVEN WONDERLANDS OF THE AMERICAN WEST\n",
      "               BY THOMAS D. MURPHY                     $6.00\n",
      "\n",
      "     A WONDERLAND OF THE EAST: The Mountain and Lake Region of\n",
      "     New England and Eastern New York\n",
      "               BY WILLIAM COPEMAN KITCHIN, PH.D.       $6.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look into the text RDD\n",
    "for line in highway.take(50):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Samsung’s Galaxy S11 is close to release and its supersized battery, upgraded biometrics and class-leading new display look set to make up for a surprisingly strange name change. But the real star of 2020 could be the phone Samsung accidentally just confirmed. Following a major blunder, the manual for Samsung’s Galaxy S10 Lite has been leaked to SamMobile. It reveals a phone which delivers far more than its name suggests, for significantly less than you might expect. In fact, industry insiders Ice Universe and OnLeaks have stated it will deliver elements which blow both the Galaxy S10 and Note 10 away. ']"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy and paste from Forbes about the Samsung’s Galaxy S11\n",
    "galaxy = sc.textFile(\"galaxy.txt\")\n",
    "\n",
    "print(type(galaxy))\n",
    "galaxy.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitions\n",
    "are logical divisions of a RDD and allows to distribute \n",
    "data subsets on nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default partition setting: 2\n",
      "n partitions of highway: 2\n",
      "spark_1_part: 4\n",
      "spark_dt_2: 4\n"
     ]
    }
   ],
   "source": [
    "# Default minimum number of partitions for RDDs\n",
    "print(\"default partition setting:\", sc.defaultMinPartitions)\n",
    "\n",
    "# n of highway\n",
    "n_part_high = highway.getNumPartitions()\n",
    "print(\"n partitions of highway: {}\".format(n_part_high))\n",
    "\n",
    "spark_1_part = spark_dt_1.getNumPartitions()\n",
    "print(\"spark_1_part:\", spark_1_part )\n",
    "\n",
    "spark_2_part = spark_dt_2.getNumPartitions()\n",
    "print(\"spark_dt_2:\", spark_2_part )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n partitions of highway set with min: 5\n"
     ]
    }
   ],
   "source": [
    "# partitions are set with minPartitions\n",
    "highway_2 = sc.textFile('OnSunsetHighways.txt', minPartitions=5 )\n",
    "n_part_high2 = highway_2.getNumPartitions()\n",
    "\n",
    "print(\"n partitions of highway set with min: {}\".format(n_part_high2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD operations\n",
    "\n",
    "<p>\n",
    "in Spark are dividied between transformation and actions. \n",
    "Transformations create new data frames and return pointers. Actions conduct computations with RDD data and return results.\n",
    "Spark creates a graph of all transformation and execution is\n",
    "only done, when an action is triggered. This is called\n",
    "\"lazy evaluation\" and the reason for the robustness of Spark.\n",
    "</p> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD operations with tabular structured numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "\n",
      "first five of thick:  [5, 5, 3, 6, 4]\n",
      "first five of thick power:  [25, 25, 9, 36, 16]\n",
      "n observations thick:  699\n",
      "\n",
      "25\n",
      "25\n",
      "9\n",
      "36\n",
      "16\n",
      "\n",
      "greater 10:  [25, 25, 36, 64, 25]\n"
     ]
    }
   ],
   "source": [
    "# Transformation\n",
    "thick_power = thick.map(lambda j: j**2)\n",
    "\n",
    "print(type(thick_power))\n",
    "print(type(thick))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Actions\n",
    "print(\"first five of thick: \", thick.take(5))\n",
    "print(\"first five of thick power: \", thick_power.take(5))\n",
    "\n",
    "print(\"n observations thick: \", thick.count())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "for i in thick_power.take(5):\n",
    "    print(i)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# transformation\n",
    "thick_p = thick_power.filter(lambda k: k > 20)\n",
    "\n",
    "# action\n",
    "print(\"greater 10: \", thick_p.take(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 2), (4, 102), (1, 193), (2, 141), (6, 24)]\n",
      "\n",
      "[(6, 199), (6, 198), (6, 194), (6, 190), (6, 188)]\n",
      "\n",
      "[(4, 102), (1, 193), (6, 24), (5, 173), (6, 140)]\n",
      "\n",
      "[3, 4, 1, 2, 6]\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Selecting data\n",
    "print(RDD_tuple.take(5))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Take top 5 RDD elements\n",
    "print(RDD_tuple.top(5))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Return distinct RDD values\n",
    "print(RDD_tuple.distinct().take(5))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Return (key,value) RDD's keys\n",
    "print(RDD_tuple.keys().take(5))\n",
    "\n",
    "# print the first 5 keys\n",
    "for i in RDD_tuple.take(5):\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RDD operations on text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text RDD: highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# highway is an unstructured text file\n",
    "print(type(highway))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_lines(lines):\n",
    "    lines = lines.lower()\n",
    "    lines = lines.split()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "['on', 'sunset', 'highways', '***', 'produced', 'by', 'melissa', 'mcdaniel', 'and', 'the', 'online', 'distributed', 'proofreading', 'team', 'at', 'http://www.pgdp.net', '(this', 'file', 'was', 'produced', 'from', 'images', 'generously', 'made', 'available', 'by', 'the', 'internet', 'archive.', 'the', 'map', 'and', 'cover', 'are', 'courtesy', 'of', 'the', 'california', 'history', 'room,', 'california', 'state', 'library,', 'sacramento,', 'california.)', \"transcriber's\", 'note:', 'inconsistent', 'hyphenation', 'in']\n"
     ]
    }
   ],
   "source": [
    "# flatmap returns multiple values for each element in the original RDD\n",
    "# and returns a transformed RDD\n",
    "\n",
    "highway_mc = highway.flatMap(correct_lines)\n",
    "print(type(highway_mc))\n",
    "\n",
    "# action\n",
    "print(highway_mc.take(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['climate', 'climate', 'climate', 'climate,', 'climate']\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "climate = highway_mc.filter(lambda line: 'climate' in line)\n",
    "print(climate.take(5))\n",
    "print(climate.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dictionary of word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "[('', 6601), ('***', 1), ('Distributed', 1), ('at', 493), ('was', 854), ('produced', 6), ('generously', 1), ('The', 682), ('Archive.', 1), ('are', 458), ('of', 4282), ('State', 12), ('Library,', 1), ('Note:', 1), ('in', 1792), ('preserved.', 2), ('errors', 1), ('have', 314), ('corrected.', 1), ('AMERICA', 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# making a function because word count will be done more than two times\n",
    "def wordcount(rdd):\n",
    "    \n",
    "    rdd_wc = rdd.flatMap(lambda line: line.split(\" \")) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda i, m: i + m)\n",
    "    \n",
    "# reduce by key combines values with the same key\n",
    "# map creates here a key-value-pair of form (word, 1)\n",
    "\n",
    "    return rdd_wc\n",
    "\n",
    "\n",
    "count_highway = wordcount(highway)\n",
    "\n",
    "print(type(count_highway))\n",
    "print(count_highway.take(20))\n",
    "\n",
    "# was appeared 854 in the text\n",
    "# in text analysis 'was' is considered a stop word\n",
    "# which does not contribute to the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the 10 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'list'>\n",
      "[(7929, 'the'), (6601, ''), (4282, 'of'), (3355, 'and'), (2447, 'a'), (2157, 'to'), (1792, 'in'), (1155, 'is'), (1021, 'we'), (854, 'was')]\n",
      "\n",
      "The word \"the\" appears 7929 times.\n",
      "The word \"\" appears 6601 times.\n",
      "The word \"of\" appears 4282 times.\n",
      "The word \"and\" appears 3355 times.\n",
      "The word \"a\" appears 2447 times.\n",
      "The word \"to\" appears 2157 times.\n",
      "The word \"in\" appears 1792 times.\n",
      "The word \"is\" appears 1155 times.\n",
      "The word \"we\" appears 1021 times.\n",
      "The word \"was\" appears 854 times.\n"
     ]
    }
   ],
   "source": [
    "# key-values-swap places values at first and then keys\n",
    "# the values become keys\n",
    "# keys are sorted with sortByKey\n",
    "# more on sortByKey below\n",
    "\n",
    "count_highway_swap = count_highway.map(lambda x: (x[1], x[0]))\n",
    "count_highway_sort = count_highway_swap.sortByKey(ascending = False)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "topten_1 = count_highway_sort.take(10)\n",
    "print(type(topten_1))\n",
    "print(topten_1)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "for i in count_highway_sort.take(10):\n",
    "    print(\"The word \\\"{}\\\" appears {} times.\".format(i[1], i[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function for the top-ten words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is the wordcount rdd\n",
    "# both functions could be integrated into one function\n",
    "def topten(wordcount_rdd):\n",
    "    \n",
    "    wordcount_rdd_swap = wordcount_rdd.map(lambda x: (x[1], x[0]))\n",
    "    wordcount_rdd_sort = wordcount_rdd_swap.sortByKey(ascending = False)\n",
    "\n",
    "\n",
    "    for i in wordcount_rdd_sort.take(10):\n",
    "        print(\"The word \\\"{}\\\" appears {} times.\".format(i[1], i[0]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"the\" appears 7929 times.\n",
      "The word \"\" appears 6601 times.\n",
      "The word \"of\" appears 4282 times.\n",
      "The word \"and\" appears 3355 times.\n",
      "The word \"a\" appears 2447 times.\n",
      "The word \"to\" appears 2157 times.\n",
      "The word \"in\" appears 1792 times.\n",
      "The word \"is\" appears 1155 times.\n",
      "The word \"we\" appears 1021 times.\n",
      "The word \"was\" appears 854 times.\n"
     ]
    }
   ],
   "source": [
    "topten(count_highway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text RDD: galaxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samsung’s\n",
      "galaxy\n",
      "s11\n",
      "is\n",
      "[]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# splits the text into multiple words each a line\n",
    "# flatMap flats a nested list into a one-dimensional list\n",
    "galaxy_samsung = galaxy.flatMap(correct_lines)\n",
    "\n",
    "for line in galaxy_samsung.take(4): \n",
    "  print(line)\n",
    "\n",
    "# checks the if Samsung is in the text\n",
    "samsung = galaxy_samsung.filter(lambda line: 'Samsung' in line)\n",
    "print(samsung.collect())\n",
    "print(samsung.count())\n",
    "# samsung is 3 times in the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count galaxy text: 79\n",
      "[('is', 1), ('close', 1), ('upgraded', 1), ('biometrics', 1), ('class-leading', 1), ('new', 1), ('look', 1), ('set', 1), ('make', 1), ('strange', 1), ('name', 2), ('but', 1), ('of', 1), ('2020', 1), ('phone', 2)]\n"
     ]
    }
   ],
   "source": [
    "# creating a word count\n",
    "\n",
    "galaxy_wc = wordcount(galaxy_samsung)\n",
    "\n",
    "print(\"word count galaxy text: {}\".format(galaxy_wc.count()))\n",
    "print(galaxy_wc.take(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"and\" appears 4 times.\n",
      "The word \"the\" appears 4 times.\n",
      "The word \"galaxy\" appears 3 times.\n",
      "The word \"to\" appears 3 times.\n",
      "The word \"for\" appears 3 times.\n",
      "The word \"a\" appears 3 times.\n",
      "The word \"name\" appears 2 times.\n",
      "The word \"phone\" appears 2 times.\n",
      "The word \"than\" appears 2 times.\n",
      "The word \"samsung’s\" appears 2 times.\n"
     ]
    }
   ],
   "source": [
    "topten(galaxy_wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "<p>\n",
    "There are a lot of stopwords in the text.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "In computing, stop words are words which are filtered out before processing of natural language data (text).Stop words are generally the most common words in a language.\n",
    "</p> \n",
    "\n",
    "<a href=\"https://www.w3schools.cohttps://en.wikipedia.org/wiki/Stop_wordsm\" target=\"_blank\">Wikipedia</a> \n",
    "\n",
    "<p>\n",
    "Stopwords are now filtered out. This makes for example the word count cleare.\n",
    "The english.txt ist taken from GitHub. Thanks to the contributors.\n",
    "</p> \n",
    "\n",
    "<a href=\"https://github.com/Alir3z4/stop-words/blob/master/english.txt</p> \n",
    "\" target=\"_blank\">GitHubWikipedia</a> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally a very small stopwords-list is created manually\n",
    "stopwords_small = ['is', 'cause', 'am', 'these', 'are','the','for','a', 'will']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different methods to import the stopwords-txt-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a file path with Pathlib\n",
    "path_join = pathlib.Path.cwd().joinpath(r'english.txt')\n",
    "path_join = path_join.resolve()\n",
    "\n",
    "# print(path_join)\n",
    "# print(type(path_join ))\n",
    "\n",
    "# Alternative way to create a file path with Pathlib\n",
    "# path = pathlib.Path.cwd() / 'english.txt'\n",
    "\n",
    "# print(path)\n",
    "# print(type(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'ll\", \"'tis\", \"'twas\", \"'ve\", '10', '39', 'a', \"a's\", 'able', 'ableabout']\n",
      "<class 'list'>\n",
      "\n",
      "[\"'ll\\n\", \"'tis\\n\", \"'twas\\n\", \"'ve\\n\", '10\\n']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# access the file with read\n",
    "\n",
    "with open(path_join, encoding='utf-8') as file: # Use file to refer to the file object\n",
    "    # access files\n",
    "    data1 = file.read()\n",
    "    # split strings into list\n",
    "    stopwords1 = data1.split('\\n')\n",
    "       \n",
    "print(stopwords1[:10])\n",
    "print(type(stopwords1))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# access the file with read lines\n",
    "full = os.path.join(path_join)\n",
    "with open(full, encoding=\"utf-8\") as file:\n",
    "    stopwords2 = file.readlines()\n",
    "    \n",
    "print(stopwords2[:5])\n",
    "print(type(stopwords2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(1297, 1)\n",
      "<class 'list'>\n",
      "1297\n"
     ]
    }
   ],
   "source": [
    "# alternatively using read table of Pandas\n",
    "stopwords_pd = pd.read_table(r\"english.txt\")\n",
    "print(type(stopwords_pd))\n",
    "print(stopwords_pd.shape)\n",
    "\n",
    "# to pd.Series\n",
    "stop_pd_series = pd.Series(stopwords_pd.iloc[:,0])\n",
    "\n",
    "stop_list = stop_pd_series.tolist()\n",
    "print(type(stop_list))\n",
    "print(len(stop_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "[\"'ll\", \"'tis\", \"'twas\", \"'ve\", '10']\n"
     ]
    }
   ],
   "source": [
    "# loading the stopwords text file into Pyspark\n",
    "# returns a RDD\n",
    "stopwords_RDD = sc.textFile('english.txt')\n",
    "\n",
    "print(type(stopwords_RDD))\n",
    "print(stopwords_RDD.take(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter with stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text: galaxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "['samsung’s', 'galaxy', 's11', 'close', 'to', 'release', 'and', 'its', 'supersized', 'battery,', 'upgraded', 'biometrics', 'and', 'class-leading', 'new']\n"
     ]
    }
   ],
   "source": [
    "galaxy_stop = galaxy_samsung.filter(lambda x: x not in stopwords_small)\n",
    "\n",
    "print(type(galaxy_stop))\n",
    "print(galaxy_stop.take(15))\n",
    "\n",
    "# galaxy_stop is the flattened RDD filtered of the words contained in stopwords_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "['samsung’s', 'galaxy', 's11', 'close', 'release', 'supersized', 'battery,', 'upgraded', 'biometrics', 'class-leading', 'display', 'set', 'surprisingly', 'strange', 'change.']\n"
     ]
    }
   ],
   "source": [
    "galaxy_stop2 = galaxy_samsung.filter(lambda x: x not in stopwords1)\n",
    "\n",
    "print(type(galaxy_stop2))\n",
    "print(galaxy_stop2.take(15))\n",
    "# galaxy_stop2 is the flattened RDD filtered of the words contained in stopwords1 taken from GitHub as txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: galaxy_stop contains the word \"and\", galaxy_stop2 does not. \"and\" is contained in stopwords1 and the original\n",
    "file from GitHub english.txt. It is not contained in the self created list stopwords_small. Therefore \"and\" is not\n",
    "filtered out in galaxy_stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[931] at RDD at PythonRDD.scala:53\n",
      "\n",
      "word count galaxy stop text: 45\n",
      "\n",
      "[('close', 1), ('upgraded', 1), ('biometrics', 1), ('class-leading', 1), ('set', 1), ('strange', 1), ('2020', 1), ('phone', 2), ('accidentally', 1), ('confirmed.', 1), ('major', 1), ('manual', 1), ('lite', 1), ('sammobile.', 1), ('reveals', 1)]\n",
      "\n",
      "maximum key-value-pair: ('galaxy', 3)\n",
      "minimum key-value-pair: ('close', 1)\n"
     ]
    }
   ],
   "source": [
    "galaxy_stop_wc2 = wordcount(galaxy_stop2)\n",
    "\n",
    "print(galaxy_stop_wc2)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"word count galaxy stop text: {}\".format(galaxy_stop_wc2.count()))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(galaxy_stop_wc2.take(15))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"maximum key-value-pair:\" , galaxy_stop_wc2.max(lambda x:x[1]))\n",
    "print(\"minimum key-value-pair:\" , galaxy_stop_wc2.min(lambda x:x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"galaxy\" appears 3 times.\n",
      "The word \"phone\" appears 2 times.\n",
      "The word \"samsung’s\" appears 2 times.\n",
      "The word \"s10\" appears 2 times.\n",
      "The word \"close\" appears 1 times.\n",
      "The word \"upgraded\" appears 1 times.\n",
      "The word \"biometrics\" appears 1 times.\n",
      "The word \"class-leading\" appears 1 times.\n",
      "The word \"set\" appears 1 times.\n",
      "The word \"strange\" appears 1 times.\n"
     ]
    }
   ],
   "source": [
    "topten(galaxy_stop_wc2)\n",
    "# top ten word apperances cleaned from stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text: highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "number of words:  44298\n",
      "['sunset', 'highways', '***', 'produced', 'melissa', 'mcdaniel', 'online', 'distributed', 'proofreading', 'team']\n",
      "\n",
      "PythonRDD[950] at RDD at PythonRDD.scala:53\n",
      "word key-value-pairs:  14153\n",
      "[('sunset', 28), ('***', 1), ('produced', 7), ('online', 1), ('team', 5), ('generously', 1), ('internet', 1), ('library,', 1), ('sacramento,', 4), (\"transcriber's\", 1)]\n",
      "maximum key-value-pair: ('san', 375)\n",
      "minimum key-value-pair: ('***', 1)\n"
     ]
    }
   ],
   "source": [
    "highway_mc_stop = highway_mc.filter(lambda i: i not in stopwords1)\n",
    "print(type(highway_mc_stop ))\n",
    "print(\"number of words: \", highway_mc_stop.count())\n",
    "print(highway_mc_stop.take(10))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "highway_mc_stop_wc = wordcount(highway_mc_stop)\n",
    "\n",
    "print(highway_mc_stop_wc)\n",
    "print(\"word key-value-pairs: \", highway_mc_stop_wc.count())\n",
    "print(highway_mc_stop_wc.take(10))\n",
    "print(\"maximum key-value-pair:\" , highway_mc_stop_wc.max(lambda x:x[1]))\n",
    "print(\"minimum key-value-pair:\" , highway_mc_stop_wc.min(lambda x:x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"san\" appears 375 times.\n",
      "The word \"road\" appears 338 times.\n",
      "The word \"miles\" appears 263 times.\n",
      "The word \"mission\" appears 192 times.\n",
      "The word \"santa\" appears 178 times.\n",
      "The word \"california\" appears 163 times.\n",
      "The word \"time\" appears 136 times.\n",
      "The word \"town\" appears 130 times.\n",
      "The word \"valley\" appears 115 times.\n",
      "The word \"mountain\" appears 112 times.\n"
     ]
    }
   ],
   "source": [
    "topten(highway_mc_stop_wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired RDDs\n",
    "<p>The word count data structure are RDDs. For example print(highway_mc_stop_wc) shows \"PythonRDD[237] at RDD\",\n",
    "but those are special RDDs using key-value-pairs like for example Python dictionaries or JSON-files do.\n",
    "The keys are the identifiers of the values.\n",
    "</p> \n",
    "\n",
    "<p>Spark offers a number of functions optimized to operate on paired RDDs:</p> \n",
    "\n",
    "\n",
    "<ul>\n",
    "  <li>reduceByKey(func): Combine values with the same key</li>\n",
    "  <li>groupByKey(): Group values with the same </li>\n",
    "  <li>sortByKey(): Return an RDD sorted by the key</li>\n",
    "  <li>join(): Join two pair RDDs based on their key</li> \n",
    "</ul> \n",
    "\n",
    "<p>\n",
    "Paired RDDs are often created from tuples, immutable structure of key-value pairs.\n",
    "Pairs of random tuples are created below and then transformed into a paired RDD.\n",
    "</p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 6), (3, 14), (4, 99), (6, 164), (2, 197), (3, 148), (2, 82), (2, 61), (5, 164), (4, 93), (1, 92), (1, 36), (5, 62), (4, 19), (1, 131)]\n"
     ]
    }
   ],
   "source": [
    "# import random\n",
    "\n",
    "random.seed()\n",
    "tuple_list = \\\n",
    "[ ( random.randint(1, 6), random.randint(0, 200) ) for k in range(300) ]\n",
    "\n",
    "print(tuple_list[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[962] at readRDDFromFile at PythonRDD.scala:247\n"
     ]
    }
   ],
   "source": [
    "# paired RDDs are often created from tuples\n",
    "# immutable structure of key-value pairs\n",
    "\n",
    "RDD_tuple = sc.parallelize(tuple_list)\n",
    "print(RDD_tuple)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.52666667 95.69333333]\n",
      "[2.58457079e+00 3.13404611e+03]\n",
      "[300. 300.]\n"
     ]
    }
   ],
   "source": [
    "summary788 = Statistics.colStats(RDD_tuple)\n",
    "\n",
    "print(summary788.mean())  # a dense vector containing the mean value for each column\n",
    "print(summary788.variance())  # column-wise variance\n",
    "print(summary788.numNonzeros())  # number of nonzeros in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of Key 6 is 6\n",
      "The value of Key 3 is 14\n",
      "The value of Key 4 is 99\n",
      "The value of Key 6 is 164\n",
      "The value of Key 2 is 197\n",
      "The value of Key 3 is 148\n",
      "The value of Key 2 is 82\n",
      "The value of Key 2 is 61\n",
      "The value of Key 5 is 164\n",
      "The value of Key 4 is 93\n"
     ]
    }
   ],
   "source": [
    "# An rdd is an row oject\n",
    "# iteration is necessary to read out the row values\n",
    "for i in RDD_tuple.take(10):\n",
    "    print(\"The value of Key {} is {}\".format(i[0], i[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 6), (3, 14), (4, 99), (6, 164), (2, 197)]\n",
      "\n",
      "[(6, 174), (6, 173), (6, 165), (6, 164), (6, 164)]\n",
      "\n",
      "[(6, 164), (5, 137), (1, 9), (4, 66), (3, 95)]\n",
      "\n",
      "[6, 3, 4, 6, 2]\n",
      "6\n",
      "3\n",
      "4\n",
      "6\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(RDD_tuple.take(5))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Take top 5 RDD elements\n",
    "print(RDD_tuple.top(5))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Return distinct RDD values\n",
    "print(RDD_tuple.distinct().take(5))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Return (key,value) RDD's keys\n",
    "print(RDD_tuple.keys().take(5))\n",
    "\n",
    "# print the first 5 keys\n",
    "for i in RDD_tuple.take(5):\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively a paired RDD created is created from the breast cancer data frame above.\n",
    "This maps a long list of observations to one key equal to the variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'Class'])\n",
      " \n",
      "[['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'Class']]\n"
     ]
    }
   ],
   "source": [
    "# using the breast cancer data set from above\n",
    "# and turning it into a dict\n",
    "bc_df_dict = bc_df_45_pd.to_dict()\n",
    "print(bc_df_dict.keys())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "# and turning it into a tuple\n",
    "bc_df_tuple = [(k,v) for k,v in  bc_df_dict.items() ]\n",
    "# tuples are accessed like lists\n",
    "tuple1_names = [[t[0] for t in bc_df_tuple]]\n",
    "print(tuple1_names )\n",
    "\n",
    "# creating a paired RDD from tuple\n",
    "RDD_tuple2 = sc.parallelize(bc_df_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:Sample code number / 1st-value: 1002945.0, 5th-value: 1017122.0\n",
      "Variable:Clump Thickness / 1st-value: 5.0, 5th-value: 8.0\n",
      "Variable:Uniformity of Cell Size / 1st-value: 4.0, 5th-value: 10.0\n",
      "Variable:Uniformity of Cell Shape / 1st-value: 4.0, 5th-value: 10.0\n",
      "Variable:Single Epithelial Cell Size / 1st-value: 5.0, 5th-value: 8.0\n",
      "Variable:Bare Nuclei / 1st-value: 7.0, 5th-value: 7.0\n",
      "Variable:Bland Chromatin / 1st-value: 10.0, 5th-value: 10.0\n",
      "Variable:Normal Nucleoli / 1st-value: 3.0, 5th-value: 9.0\n",
      "Variable:Mitoses / 1st-value: 2.0, 5th-value: 7.0\n",
      "Variable:Class / 1st-value: 1.0, 5th-value: 1.0\n"
     ]
    }
   ],
   "source": [
    "# elements of the RDD are accessed by iterating over the RDD\n",
    "# print out the variable name, the first and fifth element of every value list\n",
    "\n",
    "for j in RDD_tuple2.collect():\n",
    "    \n",
    "# susetting a tuple is similar to subsetting a list\n",
    "    print(\"Variable:{} / 1st-value: {}, 5th-value: {}\".format(j[0], j[1][1], j[1][5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between count and countByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of key-value-pairs: 10\n",
      "number of keys: 10\n",
      " \n",
      "CountByKey:  defaultdict(<class 'int'>, {'Sample code number': 1, 'Clump Thickness': 1, 'Uniformity of Cell Size': 1, 'Uniformity of Cell Shape': 1, 'Single Epithelial Cell Size': 1, 'Bare Nuclei': 1, 'Bland Chromatin': 1, 'Normal Nucleoli': 1, 'Mitoses': 1, 'Class': 1})\n"
     ]
    }
   ],
   "source": [
    "print(\"number of key-value-pairs:\", RDD_tuple2.count())\n",
    "print(\"number of keys:\", len(bc_df_dict.keys()))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "# count how how the keys appear in the tuple\n",
    "print(\"CountByKey: \", RDD_tuple2.countByKey())\n",
    "# one time every key appears\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of key-value-pairs: 300\n",
      " \n",
      "CountByKey:  defaultdict(<class 'int'>, {6: 43, 3: 55, 4: 64, 2: 49, 5: 48, 1: 41})\n"
     ]
    }
   ],
   "source": [
    "# random data\n",
    "print(\"number of key-value-pairs:\", RDD_tuple.count())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"CountByKey: \", RDD_tuple.countByKey())\n",
    "# the key 200 appears 2 times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operation: reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key or variable Sample code number has 1016277.0 Counts\n",
      "The key or variable Single Epithelial Cell Size has 1.0 Counts\n",
      "The key or variable Bland Chromatin has 4.0 Counts\n",
      "The key or variable Class has 1.0 Counts\n",
      "The key or variable Clump Thickness has 6.0 Counts\n",
      "The key or variable Uniformity of Cell Size has 8.0 Counts\n",
      "The key or variable Bare Nuclei has 3.0 Counts\n",
      "The key or variable Normal Nucleoli has 3.0 Counts\n",
      "The key or variable Uniformity of Cell Shape has 8.0 Counts\n",
      "The key or variable Mitoses has 7.0 Counts\n"
     ]
    }
   ],
   "source": [
    "# breast cancer data\n",
    "rddred1 = RDD_tuple2.reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in rddred1.collect(): \n",
    "  print(\"The key or variable {} has {} Counts\".format(num[0], num[0:][1][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of key-variable 4 is 5654\n",
      "The sum of key-variable 5 is 4741\n",
      "The sum of key-variable 1 is 3535\n",
      "The sum of key-variable 6 is 3986\n",
      "The sum of key-variable 2 is 4828\n",
      "The sum of key-variable 3 is 5964\n"
     ]
    }
   ],
   "source": [
    "# reduceByKey combines the keys and allows them to do operation like sum, subtraction ... on the values\n",
    "# in this case the values for every same key are summed up\n",
    "RDDred1 = RDD_tuple.reduceByKey(lambda h, j: h+j)\n",
    "\n",
    "for i in RDDred1.collect():\n",
    "    print(\"The sum of key-variable {} is {}\".format(i[0], i[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operation: sortbyKey\n",
    "\n",
    "does exactly what it says. It sorts the data by \"key\" (and not the values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key or variable 6 has the value: 3986\n",
      "Key or variable 5 has the value: 4741\n",
      "Key or variable 4 has the value: 5654\n",
      "Key or variable 3 has the value: 5964\n",
      "Key or variable 2 has the value: 4828\n",
      "Key or variable 1 has the value: 3535\n",
      "\n",
      "Key or variable 1 has the value: 3535\n",
      "Key or variable 2 has the value: 4828\n",
      "Key or variable 3 has the value: 5964\n",
      "Key or variable 4 has the value: 5654\n",
      "Key or variable 5 has the value: 4741\n",
      "Key or variable 6 has the value: 3986\n"
     ]
    }
   ],
   "source": [
    "RDDsort1 = RDDred1.sortByKey(ascending=False)\n",
    "\n",
    "for i in RDDsort1.collect():\n",
    "    print(\"Key or variable {} has the value: {}\".format(i[0], i[1]))\n",
    "# Sort the variable by key in desceding order from 6 to 1\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "RDDsort2 = RDDred1.sortByKey(ascending=True)\n",
    "\n",
    "for i in RDDsort2.collect():\n",
    "    print(\"Key or variable {} has the value: {}\".format(i[0], i[1]))\n",
    "# Sort the variable by key in desceding order from 6 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bare Nuclei\n",
      "Bland Chromatin\n",
      "Class\n",
      "Clump Thickness\n",
      "Mitoses\n",
      "Normal Nucleoli\n",
      "Sample code number\n",
      "Single Epithelial Cell Size\n",
      "Uniformity of Cell Shape\n",
      "Uniformity of Cell Size\n",
      " \n",
      "Uniformity of Cell Size\n",
      "Uniformity of Cell Shape\n",
      "Single Epithelial Cell Size\n",
      "Sample code number\n",
      "Normal Nucleoli\n",
      "Mitoses\n",
      "Clump Thickness\n",
      "Class\n",
      "Bland Chromatin\n",
      "Bare Nuclei\n"
     ]
    }
   ],
   "source": [
    "rddsort3 = rddred1.sortByKey()\n",
    "for p in rddsort3.collect():\n",
    "    print(p[0])\n",
    "    \n",
    "print(\" \")\n",
    "    \n",
    "rddsort3 = rddred1.sortByKey(ascending=False)\n",
    "for p in rddsort3.collect():\n",
    "    print(p[0])\n",
    "# works for text as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### countByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type <class 'pyspark.rdd.RDD'>\n",
      "\n",
      "[(10, 9997), (10, 9977), (10, 9960), (10, 9901), (10, 9599)]\n",
      "\n",
      "[(10, 1080), (5, 3077), (9, 129), (3, 8627), (3, 6387)]\n",
      "\n",
      "[3, 4, 10, 7, 5]\n"
     ]
    }
   ],
   "source": [
    "# creating a new data set of tuples just for excercise\n",
    "tuples_34 =\\\n",
    "[(random.randint(1,10), random.randint(1, 10000)) for i in range(1000)]\n",
    "# transform this into an RDD\n",
    "RDD34 = sc.parallelize(tuples_34)\n",
    "\n",
    "print(\"type\", type(RDD34))\n",
    "print(\"\")\n",
    "print(RDD34.top(5))\n",
    "print(\"\")\n",
    "# Return distinct RDD values\n",
    "print(RDD34.distinct().take(5))\n",
    "print(\"\")\n",
    "# Return (key,value) RDD's keys\n",
    "print(RDD34.keys().take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   5.456 5002.479]\n",
      "[8.36843243e+00 8.50986612e+06]\n",
      "[1000. 1000.]\n"
     ]
    }
   ],
   "source": [
    "# works here?\n",
    "summary557 = Statistics.colStats(RDD34)\n",
    "\n",
    "print(summary557.mean())  \n",
    "# a dense vector containing the mean value for each column\n",
    "print(summary557.variance())\n",
    "# column-wise variance\n",
    "print(summary557.numNonzeros())  \n",
    "# number of nonzeros in each column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {3: 99, 4: 108, 10: 91, 7: 99, 5: 96, 8: 86, 1: 116, 9: 118, 6: 101, 2: 86})\n",
      "<class 'collections.defaultdict'>\n"
     ]
    }
   ],
   "source": [
    "RDD34cby = \\\n",
    "RDD34.countByKey()\n",
    "\n",
    "print(RDD34cby)\n",
    "print(type(RDD34cby))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key 3 has 99 counts\n",
      "key 4 has 108 counts\n",
      "key 10 has 91 counts\n",
      "key 7 has 99 counts\n",
      "key 5 has 96 counts\n",
      "key 8 has 86 counts\n",
      "key 1 has 116 counts\n",
      "key 9 has 118 counts\n",
      "key 6 has 101 counts\n",
      "key 2 has 86 counts\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the total and print the output\n",
    "for k, v in RDD34cby.items(): \n",
    "  print(\"key\", k, \"has\", v, \"counts\")\n",
    "# This mean key 3 occurs 105 times in the RDD data set \"RDD34\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  groupBy & groupByKey\n",
    "\n",
    "whereby the latter works on paired RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "<bound method RDD.take of PythonRDD[1072] at RDD at PythonRDD.scala:53>\n",
      "sams ['samsung’s', 'samsung', 'samsung’s']\n",
      "rele ['release']\n",
      "batt ['battery,']\n",
      "biom ['biometrics']\n",
      "disp ['display']\n",
      "set ['set']\n",
      "stra ['strange']\n",
      "2020 ['2020']\n",
      "phon ['phone', 'phone']\n",
      "conf ['confirmed.']\n",
      "majo ['major']\n",
      "blun ['blunder,']\n",
      "manu ['manual']\n",
      "lite ['lite']\n",
      "leak ['leaked']\n",
      "samm ['sammobile.']\n",
      "reve ['reveals']\n",
      "expe ['expect.']\n",
      "insi ['insiders']\n",
      "ice ['ice']\n",
      "univ ['universe']\n",
      "onle ['onleaks']\n",
      "stat ['stated']\n",
      "away ['away.']\n",
      "gala ['galaxy', 'galaxy', 'galaxy']\n",
      "s11 ['s11']\n",
      "clos ['close']\n",
      "supe ['supersized']\n",
      "upgr ['upgraded']\n",
      "clas ['class-leading']\n"
     ]
    }
   ],
   "source": [
    "# groupBy the first 4 letters the words start with\n",
    "# this is the RDD cleaned from stop words\n",
    "galaxy_grb = galaxy_stop2.groupBy(lambda letters: letters[0:4])\n",
    "print(type(galaxy_grb))\n",
    "print(galaxy_grb.take)\n",
    "\n",
    "for k,v in galaxy_grb.take(30):\n",
    "    print(k,list(v))\n",
    "# groupBy attaches every word starting with the first 4 letters to this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the frequency of the grouped RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('close', [1]), ('upgraded', [1]), ('biometrics', [1]), ('class-leading', [1]), ('set', [1]), ('strange', [1]), ('2020', [1]), ('phone', [1, 1]), ('accidentally', [1]), ('confirmed.', [1]), ('major', [1]), ('manual', [1]), ('lite', [1]), ('sammobile.', [1]), ('reveals', [1]), ('suggests,', [1]), ('expect.', [1]), ('insiders', [1]), ('ice', [1]), ('universe', [1])]\n",
      "\n",
      "Mapped values:  [(3, 'galaxy'), (2, 'phone'), (2, 'samsung’s'), (2, 's10'), (1, 'close'), (1, 'upgraded'), (1, 'biometrics'), (1, 'class-leading'), (1, 'set'), (1, 'strange')]\n"
     ]
    }
   ],
   "source": [
    "# this transforms the RDD into a paired RDD consisting of key-value-pairs\n",
    "galaxy_paired = galaxy_stop2.map(lambda word: (word, 1))\n",
    "# now the paired RDD is grouped by the key word\n",
    "\n",
    "\n",
    "galaxy_paired_group = galaxy_paired.groupByKey()\n",
    "print([(i, list(m)) for i,m in galaxy_paired_group.take(20)])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# mapValues can aggregate, sum, count the values in every key\n",
    "# map is the key value swap need for sorting\n",
    "galaxy_mV =\\\n",
    "galaxy_paired_group.mapValues(sum).map(lambda x: (x[1],x[0])).sortByKey(ascending=False)\n",
    "print(\"Mapped values: \", galaxy_mV.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sunset', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('***', [1]), ('produced', [1, 1, 1, 1, 1, 1, 1]), ('online', [1]), ('team', [1, 1, 1, 1, 1]), ('generously', [1]), ('internet', [1]), ('library,', [1]), ('sacramento,', [1, 1, 1, 1]), (\"transcriber's\", [1])]\n",
      "\n",
      "Mapped values:  [(375, 'san'), (338, 'road'), (263, 'miles'), (192, 'mission'), (178, 'santa'), (163, 'california'), (136, 'time'), (130, 'town'), (115, 'valley'), (112, 'mountain')]\n"
     ]
    }
   ],
   "source": [
    "# this transforms the RDD into a paired RDD consisting of key-value-pairs\n",
    "highway_paired = highway_mc_stop.map(lambda word: (word, 1))\n",
    "# now the paired RDD is grouped by the key word\n",
    "\n",
    "highway_paired_group = highway_paired.groupByKey()\n",
    "print([(i, list(m)) for i,m in highway_paired_group.take(10)])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# mapValues can aggregate, sum, count the values in every key\n",
    "# map is the key value swap need for sorting\n",
    "highway_mV =\\\n",
    "highway_paired_group.mapValues(sum).map(lambda x: (x[1],x[0])).sortByKey(ascending=False)\n",
    "print(\"Mapped values: \", highway_mV.take(10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join\n",
    "\n",
    "two RDDs by keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'> <class 'pyspark.rdd.PipelinedRDD'>\n",
      "4441 5230\n"
     ]
    }
   ],
   "source": [
    "# At first making to RDDs from one RDD by taking samples\n",
    "high_sample_1 = highway_paired.sample(withReplacement=False, fraction=0.1, seed=45)\n",
    "high_sample_2 = highway_paired.sample(withReplacement=False, fraction=0.12, seed=78)\n",
    "\n",
    "print(type(high_sample_1), type(high_sample_2))\n",
    "print(len(high_sample_1.collect()), len(high_sample_2.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sunset', (1, 1)), ('sunset', (1, 1)), ('sunset', (1, 1))]\n"
     ]
    }
   ],
   "source": [
    "# join\n",
    "join_1 = high_sample_1.join(high_sample_2)\n",
    "print(join_1.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['close', 'upgraded', 'biometrics', 'class-leading', 'new']"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct1 = galaxy_stop.distinct()\n",
    "\n",
    "[i for i in distinct1.take(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
